{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2008381,"sourceType":"datasetVersion","datasetId":1201791},{"sourceId":137584,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":116476,"modelId":139700}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-21T17:00:13.360123Z","iopub.execute_input":"2024-10-21T17:00:13.360794Z","iopub.status.idle":"2024-10-21T17:00:47.954413Z","shell.execute_reply.started":"2024-10-21T17:00:13.360758Z","shell.execute_reply":"2024-10-21T17:00:47.953491Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport os\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:08:59.840682Z","iopub.execute_input":"2024-10-21T17:08:59.841572Z","iopub.status.idle":"2024-10-21T17:08:59.846810Z","shell.execute_reply.started":"2024-10-21T17:08:59.841533Z","shell.execute_reply":"2024-10-21T17:08:59.845799Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom sklearn.model_selection import train_test_split\n\n# Paths to directories\nsar_dir = \"/kaggle/input/sentinel12-image-pairs-segregated-by-terrain/v_2/urban/s1\"\noptical_dir = \"/kaggle/input/sentinel12-image-pairs-segregated-by-terrain/v_2/urban/s2\"\n\n# Get a list of image files (assuming both SAR and optical images have matching filenames)\nsar_images = sorted(os.listdir(sar_dir))\noptical_images = sorted(os.listdir(optical_dir))\n\n# Split data into training, validation, and test sets (80% train, 10% validation, 10% test)\ntrain_sar, temp_sar, train_optical, temp_optical = train_test_split(sar_images, optical_images, test_size=0.2, random_state=42)\nval_sar, test_sar, val_optical, test_optical = train_test_split(temp_sar, temp_optical, test_size=0.5, random_state=42)\n\n# Create lists of full file paths for training, validation, and test sets\ntrain_set = [(os.path.join(sar_dir, img), os.path.join(optical_dir, img)) for img in train_sar]\nval_set = [(os.path.join(sar_dir, img), os.path.join(optical_dir, img)) for img in val_sar]\ntest_set = [(os.path.join(sar_dir, img), os.path.join(optical_dir, img)) for img in test_sar]\n\n# Print the number of images in each set\nprint(f\"Training set: {len(train_set)} images\")\nprint(f\"Validation set: {len(val_set)} images\")\nprint(f\"Test set: {len(test_set)} images\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\nclass SAROpticalDataset(Dataset):\n    def __init__(self, sar_images, optical_images, sar_dir, optical_dir, transform=None):\n        self.sar_images = sar_images  # list of SAR image filenames\n        self.optical_images = optical_images  # list of Optical image filenames\n        self.sar_dir = sar_dir  # directory of SAR images\n        self.optical_dir = optical_dir  # directory of Optical images\n        self.transform = transform\n\n    def __len__(self):\n        return min(len(self.sar_images), len(self.optical_images))\n\n    def __getitem__(self, idx):\n        # Get image filenames\n        sar_image_path = os.path.join(self.sar_dir, self.sar_images[idx])\n        optical_image_path = os.path.join(self.optical_dir, self.optical_images[idx])\n        \n        # Load images\n        sar_image = Image.open(sar_image_path).convert(\"RGB\")\n        optical_image = Image.open(optical_image_path).convert(\"RGB\")\n\n        if self.transform:\n            sar_image = self.transform(sar_image)\n            optical_image = self.transform(optical_image)\n\n        return sar_image, optical_image\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create instances of the dataset\ntrain_dataset = SAROpticalDataset(train_sar, train_optical, sar_dir, optical_dir, transform=transform)\nval_dataset = SAROpticalDataset(val_sar, val_optical, sar_dir, optical_dir, transform=transform)\ntest_dataset = SAROpticalDataset(test_sar, test_optical, sar_dir, optical_dir, transform=transform)\n\n# Create DataLoader instances\nfrom torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:09:07.214421Z","iopub.execute_input":"2024-10-21T17:09:07.214817Z","iopub.status.idle":"2024-10-21T17:09:07.222002Z","shell.execute_reply.started":"2024-10-21T17:09:07.214780Z","shell.execute_reply":"2024-10-21T17:09:07.220974Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass UNetGenerator(nn.Module):\n    def __init__(self):\n        super(UNetGenerator, self).__init__()\n        \n        # Encoder\n        self.encoder1 = self.contracting_block(3, 64)    # Input: 256x256, Output: 128x128\n        self.encoder2 = self.contracting_block(64, 128)   # Output: 64x64\n        self.encoder3 = self.contracting_block(128, 256)  # Output: 32x32\n        self.encoder4 = self.contracting_block(256, 512)  # Output: 16x16\n        self.bottleneck = self.contracting_block(512, 1024)  # Output: 8x8\n\n        # Decoder\n        self.decoder4 = self.expansive_block(1024, 512) \n        self.decoder3 = self.expansive_block(1024, 256)   \n        self.decoder2 = self.expansive_block(512, 128)   \n        self.decoder1 = self.expansive_block(256, 64)     \n        \n        self.final_conv = nn.Conv2d(128, 3, kernel_size=1)  # Adjusted to take 128 input channels\n\n    def contracting_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),  # Downsampling\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_channels),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_channels)\n        )\n\n    def expansive_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),  # Upsampling\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_channels),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_channels)\n        )\n\n    def forward(self, x):\n        # Encoder\n        enc1 = self.encoder1(x)  # 128x128\n        enc2 = self.encoder2(enc1)  # 64x64\n        enc3 = self.encoder3(enc2)  # 32x32\n        enc4 = self.encoder4(enc3)  # 16x16\n        bottleneck = self.bottleneck(enc4)  # 8x8\n\n        # Decoder\n        dec4 = self.decoder4(bottleneck)  # 16x16\n        dec4 = torch.cat((dec4, enc4), dim=1)  # Skip connection\n        dec3 = self.decoder3(dec4)  # 32x32\n        dec3 = torch.cat((dec3, enc3), dim=1)  # Skip connection\n        dec2 = self.decoder2(dec3)  # 64x64\n        dec2 = torch.cat((dec2, enc2), dim=1)  # Skip connection\n        dec1 = self.decoder1(dec2)  # 128x128\n        dec1 = torch.cat((dec1, enc1), dim=1)  # Skip connection\n\n        # Final output to get 256x256\n        output = nn.functional.interpolate(dec1, size=(256, 256), mode='bilinear', align_corners=False)  # Ensure output is 256x256\n        output = self.final_conv(output)  # Output: 256x256\n        return output\n\n# Example usage\nmodel = UNetGenerator()\ninput_tensor = torch.randn(1, 3, 256, 256)  # Batch size of 1, 3 channels, 256x256 image\noutput_tensor = model(input_tensor)\nprint(\"Output tensor shape:\", output_tensor.shape)  # Should print: Output tensor shape: torch.Size([1, 3, 256, 256])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:09:10.145078Z","iopub.execute_input":"2024-10-21T17:09:10.145461Z","iopub.status.idle":"2024-10-21T17:09:10.742193Z","shell.execute_reply.started":"2024-10-21T17:09:10.145424Z","shell.execute_reply":"2024-10-21T17:09:10.741315Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(6, 64, kernel_size=4, stride=2, padding=1),  # Input: SAR + Optical\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)  # Should output (B, 1, H/16, W/16)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:09:14.150071Z","iopub.execute_input":"2024-10-21T17:09:14.150456Z","iopub.status.idle":"2024-10-21T17:09:14.159657Z","shell.execute_reply.started":"2024-10-21T17:09:14.150420Z","shell.execute_reply":"2024-10-21T17:09:14.158507Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LAMBDA = 100\n\ndef generator_loss(disc_generated_output, gen_output, target):\n    # Calculate GAN loss\n    gan_loss = F.binary_cross_entropy_with_logits(disc_generated_output, \n                                                   torch.ones_like(disc_generated_output))\n\n    # Mean absolute error\n    l1_loss = F.l1_loss(gen_output, target)\n\n    # Total generator loss\n    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n\n    return total_gen_loss, gan_loss, l1_loss","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:09:16.619243Z","iopub.execute_input":"2024-10-21T17:09:16.619644Z","iopub.status.idle":"2024-10-21T17:09:16.625670Z","shell.execute_reply.started":"2024-10-21T17:09:16.619581Z","shell.execute_reply":"2024-10-21T17:09:16.624743Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def discriminator_loss(disc_real_output, disc_generated_output):\n    # Calculate the loss for real images\n    real_loss = F.binary_cross_entropy_with_logits(disc_real_output, \n                                                   torch.ones_like(disc_real_output))\n\n    # Calculate the loss for generated images\n    generated_loss = F.binary_cross_entropy_with_logits(disc_generated_output, \n                                                         torch.zeros_like(disc_generated_output))\n\n    # Total discriminator loss\n    total_disc_loss = real_loss + generated_loss\n\n    return total_disc_loss","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:09:18.631511Z","iopub.execute_input":"2024-10-21T17:09:18.632444Z","iopub.status.idle":"2024-10-21T17:09:18.637708Z","shell.execute_reply.started":"2024-10-21T17:09:18.632404Z","shell.execute_reply":"2024-10-21T17:09:18.636550Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm  # For progress visualization\n\n# Define hyperparameters\nnum_epochs = 30# Total number of epochs you want to train\nstart_epoch = 81  # Resume from epoch 41\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize models\ngenerator = UNetGenerator().to(device)\ndiscriminator = Discriminator().to(device)\n\n# Initialize optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\n# Load checkpoint\ncheckpoint = torch.load('/kaggle/input/checkpoint_epoch_80.pth/pytorch/default/1/checkpoint_epoch_80 (1).pth')\ngenerator.load_state_dict(checkpoint['generator_state_dict'])\ndiscriminator.load_state_dict(checkpoint['discriminator_state_dict'])\noptimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\noptimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n\nprint(f\"Resuming training from epoch {start_epoch}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:09:21.303065Z","iopub.execute_input":"2024-10-21T17:09:21.303453Z","iopub.status.idle":"2024-10-21T17:09:22.223211Z","shell.execute_reply.started":"2024-10-21T17:09:21.303416Z","shell.execute_reply":"2024-10-21T17:09:22.222232Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Training loop\nnum_epochs = 30\nfor epoch in range(num_epochs):\n    generator.train()\n    discriminator.train()\n\n    for i, (sar_images, optical_images) in enumerate(train_loader):\n        # Move images to device\n        sar_images = sar_images.to(device)\n        optical_images = optical_images.to(device)\n\n        # Train Discriminator\n        optimizer_D.zero_grad()\n\n        # Create labels for real and fake images (size should match the output of the discriminator)\n        real_labels = torch.ones(sar_images.size(0), 1, 15, 15).to(device)  # Adjust size for PatchGAN\n        fake_labels = torch.zeros(sar_images.size(0), 1, 15, 15).to(device)\n\n        # Forward pass real images through discriminator\n        real_outputs = discriminator(torch.cat((sar_images, optical_images), dim=1))\n        d_loss_real = discriminator_loss(real_outputs, real_labels)\n\n        # Generate fake images using the generator\n        fake_images = generator(sar_images)\n\n        # Forward pass fake images through discriminator\n        fake_outputs = discriminator(torch.cat((sar_images, fake_images), dim=1))\n        d_loss_fake = discriminator_loss(fake_outputs, fake_labels)\n\n        # Calculate total discriminator loss and backpropagate\n        d_loss = d_loss_real + d_loss_fake\n        d_loss.backward()  # This should only be called once per iteration\n        optimizer_D.step()\n\n        # Train Generator\n        optimizer_G.zero_grad()\n        fake_images = generator(sar_images)\n        # Forward pass fake images through discriminator again for generator loss\n        fake_outputs = discriminator(torch.cat((sar_images, fake_images), dim=1))\n\n        # Calculate generator loss\n        g_loss_total, g_loss_gan, g_loss_l1 = generator_loss(fake_outputs, fake_images, optical_images)\n\n        # Backpropagate generator loss\n        g_loss_total.backward()  # Only once per iteration\n        optimizer_G.step()\n\n        if i % 50 == 0:  # Print every 50 iterations\n            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i}/{len(train_loader)}], \"\n                  f\"D Loss: {d_loss.item():.4f}, G Loss: {g_loss_total.item():.4f}\")\n\n    # Validation after each epoch\n    generator.eval()\n    with torch.no_grad():\n        val_loss = 0.0\n        for sar_images, optical_images in val_loader:\n            sar_images = sar_images.to(device)\n            optical_images = optical_images.to(device)\n\n            fake_images = generator(sar_images)\n            val_loss += F.l1_loss(fake_images, optical_images).item()\n\n    print(f\"Validation Loss after Epoch [{epoch + 1}/{num_epochs}]: {val_loss / len(val_loader):.4f}\")\n\n    # Save checkpoint after each epoch\n    checkpoint_path = f'checkpoint_epoch_{epoch + 1}.pth'  # Save with epoch number in filename\n    torch.save({\n        'epoch': epoch + 1,\n        'generator_state_dict': generator.state_dict(),\n        'discriminator_state_dict': discriminator.state_dict(),\n        'optimizer_G_state_dict': optimizer_G.state_dict(),\n        'optimizer_D_state_dict': optimizer_D.state_dict(),\n    }, checkpoint_path)\n    print(f\"Checkpoint saved at {checkpoint_path}\")\n\nprint(\"Training complete.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:15:49.441107Z","iopub.execute_input":"2024-10-21T17:15:49.441839Z","iopub.status.idle":"2024-10-21T17:16:30.153385Z","shell.execute_reply.started":"2024-10-21T17:15:49.441801Z","shell.execute_reply":"2024-10-21T17:16:30.151732Z"},"trusted":true},"outputs":[],"execution_count":null}]}