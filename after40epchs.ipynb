{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2008381,"sourceType":"datasetVersion","datasetId":1201791},{"sourceId":136871,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":115846,"modelId":139074},{"sourceId":138015,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":116862,"modelId":140080}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-21T15:08:51.515127Z","iopub.execute_input":"2024-10-21T15:08:51.515426Z","iopub.status.idle":"2024-10-21T15:09:35.757314Z","shell.execute_reply.started":"2024-10-21T15:08:51.515393Z","shell.execute_reply":"2024-10-21T15:09:35.756365Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport os\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-10-21T15:09:35.758964Z","iopub.execute_input":"2024-10-21T15:09:35.759411Z","iopub.status.idle":"2024-10-21T15:09:39.911482Z","shell.execute_reply.started":"2024-10-21T15:09:35.759359Z","shell.execute_reply":"2024-10-21T15:09:39.910632Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom sklearn.model_selection import train_test_split\n\n# Paths to directories\nsar_dir = \"/kaggle/input/sentinel12-image-pairs-segregated-by-terrain/v_2/urban/s1\"\noptical_dir = \"/kaggle/input/sentinel12-image-pairs-segregated-by-terrain/v_2/urban/s2\"\n\n# Get a list of image files (assuming both SAR and optical images have matching filenames)\nsar_images = sorted(os.listdir(sar_dir))\noptical_images = sorted(os.listdir(optical_dir))\n\n# Split data into training, validation, and test sets (80% train, 10% validation, 10% test)\ntrain_sar, temp_sar, train_optical, temp_optical = train_test_split(sar_images, optical_images, test_size=0.2, random_state=42)\nval_sar, test_sar, val_optical, test_optical = train_test_split(temp_sar, temp_optical, test_size=0.5, random_state=42)\n\n# Create lists of full file paths for training, validation, and test sets\ntrain_set = [(os.path.join(sar_dir, img), os.path.join(optical_dir, img)) for img in train_sar]\nval_set = [(os.path.join(sar_dir, img), os.path.join(optical_dir, img)) for img in val_sar]\ntest_set = [(os.path.join(sar_dir, img), os.path.join(optical_dir, img)) for img in test_sar]\n\n# Print the number of images in each set\nprint(f\"Training set: {len(train_set)} images\")\nprint(f\"Validation set: {len(val_set)} images\")\nprint(f\"Test set: {len(test_set)} images\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T15:09:39.912560Z","iopub.execute_input":"2024-10-21T15:09:39.912977Z","iopub.status.idle":"2024-10-21T15:09:40.513775Z","shell.execute_reply.started":"2024-10-21T15:09:39.912944Z","shell.execute_reply":"2024-10-21T15:09:40.512803Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\nclass SAROpticalDataset(Dataset):\n    def __init__(self, sar_images, optical_images, sar_dir, optical_dir, transform=None):\n        self.sar_images = sar_images  # list of SAR image filenames\n        self.optical_images = optical_images  # list of Optical image filenames\n        self.sar_dir = sar_dir  # directory of SAR images\n        self.optical_dir = optical_dir  # directory of Optical images\n        self.transform = transform\n\n    def __len__(self):\n        return min(len(self.sar_images), len(self.optical_images))\n\n    def __getitem__(self, idx):\n        # Get image filenames\n        sar_image_path = os.path.join(self.sar_dir, self.sar_images[idx])\n        optical_image_path = os.path.join(self.optical_dir, self.optical_images[idx])\n        \n        # Load images\n        sar_image = Image.open(sar_image_path).convert(\"RGB\")\n        optical_image = Image.open(optical_image_path).convert(\"RGB\")\n\n        if self.transform:\n            sar_image = self.transform(sar_image)\n            optical_image = self.transform(optical_image)\n\n        return sar_image, optical_image\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T15:09:40.515549Z","iopub.execute_input":"2024-10-21T15:09:40.515995Z","iopub.status.idle":"2024-10-21T15:09:40.525671Z","shell.execute_reply.started":"2024-10-21T15:09:40.515961Z","shell.execute_reply":"2024-10-21T15:09:40.524834Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create instances of the dataset\ntrain_dataset = SAROpticalDataset(train_sar, train_optical, sar_dir, optical_dir, transform=transform)\nval_dataset = SAROpticalDataset(val_sar, val_optical, sar_dir, optical_dir, transform=transform)\ntest_dataset = SAROpticalDataset(test_sar, test_optical, sar_dir, optical_dir, transform=transform)\n\n# Create DataLoader instances\nfrom torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T15:09:40.527035Z","iopub.execute_input":"2024-10-21T15:09:40.527389Z","iopub.status.idle":"2024-10-21T15:09:40.537592Z","shell.execute_reply.started":"2024-10-21T15:09:40.527347Z","shell.execute_reply":"2024-10-21T15:09:40.536715Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass UNetGenerator(nn.Module):\n    def __init__(self):\n        super(UNetGenerator, self).__init__()\n        \n        # Encoder\n        self.encoder1 = self.contracting_block(3, 64)    # Input: 256x256, Output: 128x128\n        self.encoder2 = self.contracting_block(64, 128)   # Output: 64x64\n        self.encoder3 = self.contracting_block(128, 256)  # Output: 32x32\n        self.encoder4 = self.contracting_block(256, 512)  # Output: 16x16\n        self.bottleneck = self.contracting_block(512, 1024)  # Output: 8x8\n\n        # Decoder\n        self.decoder4 = self.expansive_block(1024, 512) \n        self.decoder3 = self.expansive_block(1024, 256)   \n        self.decoder2 = self.expansive_block(512, 128)   \n        self.decoder1 = self.expansive_block(256, 64)     \n        \n        self.final_conv = nn.Conv2d(128, 3, kernel_size=1)  # Adjusted to take 128 input channels\n\n    def contracting_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),  # Downsampling\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_channels),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_channels)\n        )\n\n    def expansive_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),  # Upsampling\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_channels),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_channels)\n        )\n\n    def forward(self, x):\n        # Encoder\n        enc1 = self.encoder1(x)  # 128x128\n        enc2 = self.encoder2(enc1)  # 64x64\n        enc3 = self.encoder3(enc2)  # 32x32\n        enc4 = self.encoder4(enc3)  # 16x16\n        bottleneck = self.bottleneck(enc4)  # 8x8\n\n        # Decoder\n        dec4 = self.decoder4(bottleneck)  # 16x16\n        dec4 = torch.cat((dec4, enc4), dim=1)  # Skip connection\n        dec3 = self.decoder3(dec4)  # 32x32\n        dec3 = torch.cat((dec3, enc3), dim=1)  # Skip connection\n        dec2 = self.decoder2(dec3)  # 64x64\n        dec2 = torch.cat((dec2, enc2), dim=1)  # Skip connection\n        dec1 = self.decoder1(dec2)  # 128x128\n        dec1 = torch.cat((dec1, enc1), dim=1)  # Skip connection\n\n        # Final output to get 256x256\n        output = nn.functional.interpolate(dec1, size=(256, 256), mode='bilinear', align_corners=False)  # Ensure output is 256x256\n        output = self.final_conv(output)  # Output: 256x256\n        return output\n\n# Example usage\nmodel = UNetGenerator()\ninput_tensor = torch.randn(1, 3, 256, 256)  # Batch size of 1, 3 channels, 256x256 image\noutput_tensor = model(input_tensor)\nprint(\"Output tensor shape:\", output_tensor.shape)  # Should print: Output tensor shape: torch.Size([1, 3, 256, 256])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T15:10:24.984484Z","iopub.execute_input":"2024-10-21T15:10:24.985166Z","iopub.status.idle":"2024-10-21T15:10:25.752723Z","shell.execute_reply.started":"2024-10-21T15:10:24.985126Z","shell.execute_reply":"2024-10-21T15:10:25.751706Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(6, 64, kernel_size=4, stride=2, padding=1),  # Input: SAR + Optical\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)  # Should output (B, 1, H/16, W/16)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T12:24:41.800979Z","iopub.execute_input":"2024-10-15T12:24:41.801289Z","iopub.status.idle":"2024-10-15T12:24:41.810100Z","shell.execute_reply.started":"2024-10-15T12:24:41.801258Z","shell.execute_reply":"2024-10-15T12:24:41.808874Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LAMBDA = 100\n\ndef generator_loss(disc_generated_output, gen_output, target):\n    # Calculate GAN loss\n    gan_loss = F.binary_cross_entropy_with_logits(disc_generated_output, \n                                                   torch.ones_like(disc_generated_output))\n\n    # Mean absolute error\n    l1_loss = F.l1_loss(gen_output, target)\n\n    # Total generator loss\n    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n\n    return total_gen_loss, gan_loss, l1_loss","metadata":{"execution":{"iopub.status.busy":"2024-10-15T12:24:41.811237Z","iopub.execute_input":"2024-10-15T12:24:41.811595Z","iopub.status.idle":"2024-10-15T12:24:41.818973Z","shell.execute_reply.started":"2024-10-15T12:24:41.811547Z","shell.execute_reply":"2024-10-15T12:24:41.818013Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def discriminator_loss(disc_real_output, disc_generated_output):\n    # Calculate the loss for real images\n    real_loss = F.binary_cross_entropy_with_logits(disc_real_output, \n                                                   torch.ones_like(disc_real_output))\n\n    # Calculate the loss for generated images\n    generated_loss = F.binary_cross_entropy_with_logits(disc_generated_output, \n                                                         torch.zeros_like(disc_generated_output))\n\n    # Total discriminator loss\n    total_disc_loss = real_loss + generated_loss\n\n    return total_disc_loss","metadata":{"execution":{"iopub.status.busy":"2024-10-15T12:24:41.820144Z","iopub.execute_input":"2024-10-15T12:24:41.820501Z","iopub.status.idle":"2024-10-15T12:24:41.828990Z","shell.execute_reply.started":"2024-10-15T12:24:41.820459Z","shell.execute_reply":"2024-10-15T12:24:41.828193Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Initialize models\ngenerator = UNetGenerator().to(device)\ndiscriminator = Discriminator().to(device)\n\n# Initialize optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\n# Load checkpoint\ncheckpoint = torch.load('/kaggle/input/checkpoint_epoch_40.pth/pytorch/default/1/checkpoint_epoch_40.pth')  # Replace with the correct path to your saved checkpoint\n\n# Load model weights\ngenerator.load_state_dict(checkpoint['generator_state_dict'])\ndiscriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n\n# Load optimizer states\noptimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\noptimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n\nprint(f\"Resuming training from epoch {40}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T12:25:59.818584Z","iopub.execute_input":"2024-10-15T12:25:59.819351Z","iopub.status.idle":"2024-10-15T12:26:00.647659Z","shell.execute_reply.started":"2024-10-15T12:25:59.819312Z","shell.execute_reply":"2024-10-15T12:26:00.646716Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n# Training loop\nnum_epochs = 40\nfor epoch in range(num_epochs):\n    generator.train()\n    discriminator.train()\n\n    for i, (sar_images, optical_images) in enumerate(train_loader):\n        # Move images to device\n        sar_images = sar_images.to(device)\n        optical_images = optical_images.to(device)\n\n        # Train Discriminator\n        optimizer_D.zero_grad()\n\n        # Create labels for real and fake images (size should match the output of the discriminator)\n        real_labels = torch.ones(sar_images.size(0), 1, 15, 15).to(device)  # Adjust size for PatchGAN\n        fake_labels = torch.zeros(sar_images.size(0), 1, 15, 15).to(device)\n\n        # Forward pass real images through discriminator\n        real_outputs = discriminator(torch.cat((sar_images, optical_images), dim=1))\n        d_loss_real = discriminator_loss(real_outputs, real_labels)\n\n        # Generate fake images using the generator\n        fake_images = generator(sar_images)\n\n        # Forward pass fake images through discriminator\n        fake_outputs = discriminator(torch.cat((sar_images, fake_images), dim=1))\n        d_loss_fake = discriminator_loss(fake_outputs, fake_labels)\n\n        # Calculate total discriminator loss and backpropagate\n        d_loss = d_loss_real + d_loss_fake\n        d_loss.backward()\n        optimizer_D.step()\n\n        # Train Generator\n        optimizer_G.zero_grad()\n\n        # Generate fake images\n        fake_images = generator(sar_images)\n\n        # Forward pass fake images through discriminator\n        fake_outputs = discriminator(torch.cat((sar_images, fake_images), dim=1))\n\n        # Calculate generator loss\n        g_loss_total, g_loss_gan, g_loss_l1 = generator_loss(fake_outputs, fake_images, optical_images)\n\n        # Backpropagate generator loss\n        g_loss_total.backward()\n        optimizer_G.step()\n\n        if i % 50 == 0:  # Print every 100 iterations\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_loader)}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss_total.item():.4f}\")\n\n    # Validation after each epoch\n    generator.eval()\n    with torch.no_grad():\n        val_loss = 0.0\n        for sar_images, optical_images in val_loader:\n            sar_images = sar_images.to(device)\n            optical_images = optical_images.to(device)\n\n            fake_images = generator(sar_images)\n            val_loss += F.l1_loss(fake_images, optical_images).item()\n\n    print(f\"Validation Loss after Epoch [{epoch+1}/{num_epochs}]: {val_loss / len(val_loader):.4f}\")\n    \n    checkpoint_path = f'checkpoint_epoch_{epoch + 1}.pth'  # Save with epoch number in filename\n    torch.save({\n        'epoch': epoch + 1,\n        'generator_state_dict': generator.state_dict(),\n        'discriminator_state_dict': discriminator.state_dict(),\n        'optimizer_G_state_dict': optimizer_G.state_dict(),\n        'optimizer_D_state_dict': optimizer_D.state_dict(),\n    }, checkpoint_path)\n    print(f\"Checkpoint saved at {checkpoint_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T12:26:04.065672Z","iopub.execute_input":"2024-10-15T12:26:04.066315Z","iopub.status.idle":"2024-10-15T14:14:34.849819Z","shell.execute_reply.started":"2024-10-15T12:26:04.066275Z","shell.execute_reply":"2024-10-15T14:14:34.848838Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Load the trained generator model\ncheckpoint = torch.load(\"/kaggle/working/checkpoint_epoch_40.pth\")  # Load the entire checkpoint\ngenerator = UNetGenerator().to(device)  # Initialize the model\n\n# Load only the generator's state_dict\ngenerator.load_state_dict(checkpoint['generator_state_dict'])  # Load the state_dict for the generator\ngenerator.eval()\n\n# Define the preprocessing transformations (same as training)\npreprocess = transforms.Compose([\n    transforms.Resize((256, 256)),  # Change this if your model uses a different size\n    transforms.Lambda(lambda x: x.convert('RGB')),  # Convert to RGB (3 channels)\n    transforms.ToTensor(),\n])\n\n# Function to perform inference using a DataLoader\ndef infer_sar_to_optical(test_loader, num_images=5):\n    with torch.no_grad():  # No need to track gradients during inference\n        for i, (sar_images, optical_images) in enumerate(test_loader):  # Get SAR and original optical images from the DataLoader\n            sar_images = sar_images.to(device)  # Move SAR images to the appropriate device\n            \n            generated_images = generator(sar_images)  # Forward pass through the generator\n            \n            # Post-process the output\n            generated_images = generated_images.squeeze().cpu()  # Remove batch dimension and move to CPU\n            generated_images = (generated_images * 0.5 + 0.5)  # Denormalize (scale to [0, 1])\n            generated_images = generated_images.clamp(0, 1)  # Clamp values to valid range\n            \n            # Display original SAR, original optical, and generated images side by side\n            for sar_img, orig_optical_img, gen_img in zip(sar_images.cpu(), optical_images.cpu(), generated_images):\n                sar_img_np = sar_img.squeeze().numpy().transpose(1, 2, 0)  # Convert SAR image to HWC format\n                orig_optical_img_np = orig_optical_img.squeeze().numpy().transpose(1, 2, 0)  # Convert original optical image to HWC format\n                gen_img_np = gen_img.numpy().transpose(1, 2, 0)  # Convert generated image to HWC format\n\n                # Create a figure to display images\n                plt.figure(figsize=(15, 5))  # Set the figure size\n                plt.subplot(1, 3, 1)  # First subplot for SAR image\n                plt.imshow(sar_img_np)  # Display the SAR image\n                plt.title('Original SAR Image')\n                plt.axis('off')  # Hide axis\n\n                plt.subplot(1, 3, 2)  # Second subplot for original optical image\n                plt.imshow(orig_optical_img_np)  # Display the original optical image\n                plt.title('Original Optical Image')\n                plt.axis('off')  # Hide axis\n\n                plt.subplot(1, 3, 3)  # Third subplot for generated image\n                plt.imshow(gen_img_np)  # Display the generated image\n                plt.title('Generated Optical Image')\n                plt.axis('off')  # Hide axis\n\n                plt.show()  # Show the figure\n\n            if i + 1 >= num_images:  # Stop after processing the desired number of images\n                break\n\n# Example usage with your test DataLoader\ninfer_sar_to_optical(val_loader, num_images=5)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:24:28.035889Z","iopub.execute_input":"2024-10-15T14:24:28.036284Z","iopub.status.idle":"2024-10-15T14:25:13.388721Z","shell.execute_reply.started":"2024-10-15T14:24:28.036247Z","shell.execute_reply":"2024-10-15T14:25:13.387840Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Load the trained generator model\ncheckpoint = torch.load(\"/kaggle/input/checkpoint_epoch_80-1/pytorch/default/1/checkpoint_epoch_80 (1).pth\")  # Load the entire checkpoint\ngenerator = UNetGenerator().to(device)  # Initialize the model\n\n# Load only the generator's state_dict\ngenerator.load_state_dict(checkpoint['generator_state_dict'])  # Load the state_dict for the generator\ngenerator.eval()\n\n# Define the preprocessing transformations (same as training)\npreprocess = transforms.Compose([\n    transforms.Resize((256, 256)),  # Change this if your model uses a different size\n    transforms.Lambda(lambda x: x.convert('RGB')),  # Convert to RGB (3 channels)\n    transforms.ToTensor(),\n])\n\n# Function to perform inference using a DataLoader\ndef infer_sar_to_optical(test_loader, num_images=5):\n    with torch.no_grad():  # No need to track gradients during inference\n        for i, (sar_images, optical_images) in enumerate(test_loader):  # Get SAR and original optical images from the DataLoader\n            sar_images = sar_images.to(device)  # Move SAR images to the appropriate device\n            \n            generated_images = generator(sar_images)  # Forward pass through the generator\n            \n            # Post-process the output\n            generated_images = generated_images.squeeze().cpu()  # Remove batch dimension and move to CPU\n            generated_images = (generated_images * 0.5 + 0.5)  # Denormalize (scale to [0, 1])\n            generated_images = generated_images.clamp(0, 1)  # Clamp values to valid range\n            \n            # Display original SAR, original optical, and generated images side by side\n            for sar_img, orig_optical_img, gen_img in zip(sar_images.cpu(), optical_images.cpu(), generated_images):\n                sar_img_np = sar_img.squeeze().numpy().transpose(1, 2, 0)  # Convert SAR image to HWC format\n                orig_optical_img_np = orig_optical_img.squeeze().numpy().transpose(1, 2, 0)  # Convert original optical image to HWC format\n                gen_img_np = gen_img.numpy().transpose(1, 2, 0)  # Convert generated image to HWC format\n\n                # Create a figure to display images\n                plt.figure(figsize=(15, 5))  # Set the figure size\n                plt.subplot(1, 3, 1)  # First subplot for SAR image\n                plt.imshow(sar_img_np)  # Display the SAR image\n                plt.title('Original SAR Image')\n                plt.axis('off')  # Hide axis\n\n                plt.subplot(1, 3, 2)  # Second subplot for original optical image\n                plt.imshow(orig_optical_img_np)  # Display the original optical image\n                plt.title('Original Optical Image')\n                plt.axis('off')  # Hide axis\n\n                plt.subplot(1, 3, 3)  # Third subplot for generated image\n                plt.imshow(gen_img_np)  # Display the generated image\n                plt.title('Generated Optical Image')\n                plt.axis('off')  # Hide axis\n\n                plt.show()  # Show the figure\n\n            if i + 1 >= num_images:  # Stop after processing the desired number of images\n                break\n\n# Example usage with your test DataLoader\ninfer_sar_to_optical(test_loader, num_images=5)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T15:16:11.814894Z","iopub.execute_input":"2024-10-21T15:16:11.815286Z","iopub.status.idle":"2024-10-21T15:16:56.373544Z","shell.execute_reply.started":"2024-10-21T15:16:11.815247Z","shell.execute_reply":"2024-10-21T15:16:56.372260Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\n\n# Load the trained generator model\ncheckpoint = torch.load(\"/kaggle/input/checkpoint_epoch_80-1/pytorch/default/1/checkpoint_epoch_80 (1).pth\")  # Load the entire checkpoint\ngenerator = UNetGenerator().to(device)  # Initialize the model\n\n# Load only the generator's state_dict\ngenerator.load_state_dict(checkpoint['generator_state_dict'])  # Load the state_dict for the generator\ngenerator.eval()\n\n\n\n\n       \n# Function to perform inference\ndef infer_sar_to_optical(test_loader, num_images=5):\n    generator.eval()  # Set the generator to evaluation mode\n    with torch.no_grad():  # Disable gradient computation for inference\n         for i, (sar_images, optical_images) in enumerate(test_loader):   # Iterate through the test DataLoader\n            sar_images = sar_images.to(device)  # Move SAR images to the appropriate device\n            \n            # Generate optical images from SAR images\n            generated_images = generator(sar_images)  # Forward pass through the generator\n            \n            # Post-process the output\n            generated_images = generated_images.squeeze().cpu()  # Remove batch dimension and move to CPU\n            generated_images = generated_images.clamp(0, 1)  # Clamp values to the valid range [0, 1]\n\n            # Display SAR and generated images side by side\n            for sar_img, gen_img in zip(sar_images.cpu(), generated_images):\n                sar_img_np = sar_img.squeeze().numpy().transpose(1, 2, 0)  # Convert SAR image to HWC format\n                gen_img_np = gen_img.numpy().transpose(1, 2, 0)  # Convert generated image to HWC format\n\n                # Create a figure to display images\n                plt.figure(figsize=(10, 5))  # Set figure size\n                plt.subplot(1, 2, 1)  # First subplot for SAR image\n                plt.imshow(sar_img_np)  # Display SAR image\n                plt.title('Original SAR Image')\n                plt.axis('off')  # Hide axis\n\n                plt.subplot(1, 2, 2)  # Second subplot for generated optical image\n                plt.imshow(gen_img_np)  # Display generated optical image\n                plt.title('Generated Optical Image')\n                plt.axis('off')  # Hide axis\n\n                plt.show()  # Show the figure\n\n            if i + 1 >= num_images:  # Stop after processing the desired number of images\n                break\n\n\n\n\n# Run inference on the test dataset\ninfer_sar_to_optical(test_loader, num_images=5)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T15:15:03.607470Z","iopub.execute_input":"2024-10-21T15:15:03.608265Z","iopub.status.idle":"2024-10-21T15:15:38.291746Z","shell.execute_reply.started":"2024-10-21T15:15:03.608225Z","shell.execute_reply":"2024-10-21T15:15:38.290811Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\n\n# Load the trained generator model\ncheckpoint = torch.load(\"/kaggle/input/checkpoint_epoch_80-1/pytorch/default/1/checkpoint_epoch_80 (1).pth\")  # Load the entire checkpoint\ngenerator = UNetGenerator().to(device)  # Initialize the model\n\n# Load only the generator's state_dict\ngenerator.load_state_dict(checkpoint['generator_state_dict'])  # Load the state_dict for the generator\ngenerator.eval()\n\n# Function to perform inference and display SAR, original optical, and generated optical images\ndef infer_sar_to_optical(test_loader, num_images=5):\n    generator.eval()  # Set the generator to evaluation mode\n    with torch.no_grad():  # Disable gradient computation for inference\n        for i, (sar_images, optical_images) in enumerate(test_loader):  # Iterate through the test DataLoader\n            sar_images = sar_images.to(device)  # Move SAR images to the appropriate device\n            \n            # Generate optical images from SAR images\n            generated_images = generator(sar_images)  # Forward pass through the generator\n            \n            # Post-process the output\n            generated_images = generated_images.squeeze().cpu()  # Remove batch dimension and move to CPU\n            generated_images = generated_images.clamp(0, 1)  # Clamp values to the valid range [0, 1]\n\n            # Display SAR, original optical, and generated optical images side by side\n            for sar_img, orig_optical_img, gen_img in zip(sar_images.cpu(), optical_images.cpu(), generated_images):\n                sar_img_np = sar_img.squeeze().numpy().transpose(1, 2, 0)  # Convert SAR image to HWC format\n                orig_optical_img_np = orig_optical_img.squeeze().numpy().transpose(1, 2, 0)  # Convert original optical image to HWC format\n                gen_img_np = gen_img.numpy().transpose(1, 2, 0)  # Convert generated image to HWC format\n\n                # Create a figure to display images\n                plt.figure(figsize=(15, 5))  # Set the figure size\n\n                plt.subplot(1, 3, 1)  # First subplot for SAR image\n                plt.imshow(sar_img_np)  # Display SAR image\n                plt.title('Original SAR Image')\n                plt.axis('off')  # Hide axis\n\n                plt.subplot(1, 3, 2)  # Second subplot for original optical image\n                plt.imshow(orig_optical_img_np)  # Display original optical image\n                plt.title('Original Optical Image')\n                plt.axis('off')  # Hide axis\n\n                plt.subplot(1, 3, 3)  # Third subplot for generated optical image\n                plt.imshow(gen_img_np)  # Display generated optical image\n                plt.title('Generated Optical Image')\n                plt.axis('off')  # Hide axis\n\n                plt.show()  # Show the figure\n\n            if i + 1 >= num_images:  # Stop after processing the desired number of images\n                break\n\n# Example usage (assuming test_loader is defined)\ninfer_sar_to_optical(test_loader, num_images=5)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T15:18:54.007612Z","iopub.execute_input":"2024-10-21T15:18:54.008028Z","iopub.status.idle":"2024-10-21T15:19:39.378640Z","shell.execute_reply.started":"2024-10-21T15:18:54.007988Z","shell.execute_reply":"2024-10-21T15:19:39.377596Z"},"trusted":true},"outputs":[],"execution_count":null}]}