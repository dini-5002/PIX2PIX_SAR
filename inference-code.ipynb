{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2008381,"sourceType":"datasetVersion","datasetId":1201791},{"sourceId":142277,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":120531,"modelId":143735},{"sourceId":142430,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":120660,"modelId":143864}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-21T18:52:14.645502Z","iopub.execute_input":"2024-10-21T18:52:14.645830Z","iopub.status.idle":"2024-10-21T18:52:49.182198Z","shell.execute_reply.started":"2024-10-21T18:52:14.645795Z","shell.execute_reply":"2024-10-21T18:52:49.181147Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport os\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-10-21T18:52:49.184364Z","iopub.execute_input":"2024-10-21T18:52:49.184940Z","iopub.status.idle":"2024-10-21T18:52:54.678892Z","shell.execute_reply.started":"2024-10-21T18:52:49.184893Z","shell.execute_reply":"2024-10-21T18:52:54.677839Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom sklearn.model_selection import train_test_split\n\n# Paths to directories\nsar_dir = \"/kaggle/input/sentinel12-image-pairs-segregated-by-terrain/v_2/urban/s1\"\noptical_dir = \"/kaggle/input/sentinel12-image-pairs-segregated-by-terrain/v_2/urban/s2\"\n\n# Get a list of image files (assuming both SAR and optical images have matching filenames)\nsar_images = sorted(os.listdir(sar_dir))\noptical_images = sorted(os.listdir(optical_dir))\n\n# Split data into training, validation, and test sets (80% train, 10% validation, 10% test)\ntrain_sar, temp_sar, train_optical, temp_optical = train_test_split(sar_images, optical_images, test_size=0.2, random_state=42)\nval_sar, test_sar, val_optical, test_optical = train_test_split(temp_sar, temp_optical, test_size=0.5, random_state=42)\n\n# Create lists of full file paths for training, validation, and test sets\ntrain_set = [(os.path.join(sar_dir, img), os.path.join(optical_dir, img)) for img in train_sar]\nval_set = [(os.path.join(sar_dir, img), os.path.join(optical_dir, img)) for img in val_sar]\ntest_set = [(os.path.join(sar_dir, img), os.path.join(optical_dir, img)) for img in test_sar]\n\n# Print the number of images in each set\nprint(f\"Training set: {len(train_set)} images\")\nprint(f\"Validation set: {len(val_set)} images\")\nprint(f\"Test set: {len(test_set)} images\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T18:52:54.680225Z","iopub.execute_input":"2024-10-21T18:52:54.680727Z","iopub.status.idle":"2024-10-21T18:52:55.339110Z","shell.execute_reply.started":"2024-10-21T18:52:54.680692Z","shell.execute_reply":"2024-10-21T18:52:55.338107Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\nclass SAROpticalDataset(Dataset):\n    def __init__(self, sar_images, optical_images, sar_dir, optical_dir, transform=None):\n        self.sar_images = sar_images  # list of SAR image filenames\n        self.optical_images = optical_images  # list of Optical image filenames\n        self.sar_dir = sar_dir  # directory of SAR images\n        self.optical_dir = optical_dir  # directory of Optical images\n        self.transform = transform\n\n    def __len__(self):\n        return min(len(self.sar_images), len(self.optical_images))\n\n    def __getitem__(self, idx):\n        # Get image filenames\n        sar_image_path = os.path.join(self.sar_dir, self.sar_images[idx])\n        optical_image_path = os.path.join(self.optical_dir, self.optical_images[idx])\n        \n        # Load images\n        sar_image = Image.open(sar_image_path).convert(\"RGB\")\n        optical_image = Image.open(optical_image_path).convert(\"RGB\")\n\n        if self.transform:\n            sar_image = self.transform(sar_image)\n            optical_image = self.transform(optical_image)\n\n        return sar_image, optical_image\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T18:52:55.341988Z","iopub.execute_input":"2024-10-21T18:52:55.342730Z","iopub.status.idle":"2024-10-21T18:52:55.352620Z","shell.execute_reply.started":"2024-10-21T18:52:55.342690Z","shell.execute_reply":"2024-10-21T18:52:55.351570Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create instances of the dataset\ntrain_dataset = SAROpticalDataset(train_sar, train_optical, sar_dir, optical_dir, transform=transform)\nval_dataset = SAROpticalDataset(val_sar, val_optical, sar_dir, optical_dir, transform=transform)\ntest_dataset = SAROpticalDataset(test_sar, test_optical, sar_dir, optical_dir, transform=transform)\n\n# Create DataLoader instances\nfrom torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T18:52:55.353773Z","iopub.execute_input":"2024-10-21T18:52:55.354103Z","iopub.status.idle":"2024-10-21T18:52:55.369461Z","shell.execute_reply.started":"2024-10-21T18:52:55.354068Z","shell.execute_reply":"2024-10-21T18:52:55.368512Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass UNetGenerator(nn.Module):\n    def __init__(self):\n        super(UNetGenerator, self).__init__()\n        \n        # Encoder\n        self.encoder1 = self.contracting_block(3, 64)    # Input: 256x256, Output: 128x128\n        self.encoder2 = self.contracting_block(64, 128)   # Output: 64x64\n        self.encoder3 = self.contracting_block(128, 256)  # Output: 32x32\n        self.encoder4 = self.contracting_block(256, 512)  # Output: 16x16\n        self.bottleneck = self.contracting_block(512, 1024)  # Output: 8x8\n\n        # Decoder\n        self.decoder4 = self.expansive_block(1024, 512) \n        self.decoder3 = self.expansive_block(1024, 256)   \n        self.decoder2 = self.expansive_block(512, 128)   \n        self.decoder1 = self.expansive_block(256, 64)     \n        \n        self.final_conv = nn.Conv2d(128, 3, kernel_size=1)  # Adjusted to take 128 input channels\n\n    def contracting_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),  # Downsampling\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_channels),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_channels)\n        )\n\n    def expansive_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),  # Upsampling\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_channels),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_channels)\n        )\n\n    def forward(self, x):\n        # Encoder\n        enc1 = self.encoder1(x)  # 128x128\n        enc2 = self.encoder2(enc1)  # 64x64\n        enc3 = self.encoder3(enc2)  # 32x32\n        enc4 = self.encoder4(enc3)  # 16x16\n        bottleneck = self.bottleneck(enc4)  # 8x8\n\n        # Decoder\n        dec4 = self.decoder4(bottleneck)  # 16x16\n        dec4 = torch.cat((dec4, enc4), dim=1)  # Skip connection\n        dec3 = self.decoder3(dec4)  # 32x32\n        dec3 = torch.cat((dec3, enc3), dim=1)  # Skip connection\n        dec2 = self.decoder2(dec3)  # 64x64\n        dec2 = torch.cat((dec2, enc2), dim=1)  # Skip connection\n        dec1 = self.decoder1(dec2)  # 128x128\n        dec1 = torch.cat((dec1, enc1), dim=1)  # Skip connection\n\n        # Final output to get 256x256\n        output = nn.functional.interpolate(dec1, size=(256, 256), mode='bilinear', align_corners=False)  # Ensure output is 256x256\n        output = self.final_conv(output)  # Output: 256x256\n        return output\n\n# Example usage\nmodel = UNetGenerator()\ninput_tensor = torch.randn(1, 3, 256, 256)  # Batch size of 1, 3 channels, 256x256 image\noutput_tensor = model(input_tensor)\nprint(\"Output tensor shape:\", output_tensor.shape)  # Should print: Output tensor shape: torch.Size([1, 3, 256, 256])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T18:52:55.370692Z","iopub.execute_input":"2024-10-21T18:52:55.371025Z","iopub.status.idle":"2024-10-21T18:52:56.306019Z","shell.execute_reply.started":"2024-10-21T18:52:55.370991Z","shell.execute_reply":"2024-10-21T18:52:56.304774Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import torchvision.transforms as transforms\n# import matplotlib.pyplot as plt\n# from PIL import Image\n# import os\n\n# # Load the trained generator model\n# checkpoint = torch.load(\"/kaggle/input/checkpoint_epoch_80_.pth/pytorch/default/1/checkpoint_epoch_80 (1).pth\")  # Load the entire checkpoint\n# generator = UNetGenerator().to(device)  # Initialize the model\n\n# # Load only the generator's state_dict\n# generator.load_state_dict(checkpoint['generator_state_dict'])  # Load the state_dict for the generator\n# generator.eval()\n\n# # Function to perform inference and display SAR, original optical, and generated optical images\n# def infer_sar_to_optical(test_loader, num_images=5):\n#     generator.eval()  # Set the generator to evaluation mode\n#     with torch.no_grad():  # Disable gradient computation for inference\n#         for i, (sar_images, optical_images) in enumerate(test_loader):  # Iterate through the test DataLoader\n#             sar_images = sar_images.to(device)  # Move SAR images to the appropriate device\n            \n#             # Generate optical images from SAR images\n#             generated_images = generator(sar_images)  # Forward pass through the generator\n            \n#             # Post-process the output\n#             generated_images = generated_images.squeeze().cpu()  # Remove batch dimension and move to CPU\n#             generated_images = generated_images.clamp(0, 1)  # Clamp values to the valid range [0, 1]\n\n#             # Display SAR, original optical, and generated optical images side by side\n#             for sar_img, orig_optical_img, gen_img in zip(sar_images.cpu(), optical_images.cpu(), generated_images):\n#                 sar_img_np = sar_img.squeeze().numpy().transpose(1, 2, 0)  # Convert SAR image to HWC format\n#                 orig_optical_img_np = orig_optical_img.squeeze().numpy().transpose(1, 2, 0)  # Convert original optical image to HWC format\n#                 gen_img_np = gen_img.numpy().transpose(1, 2, 0)  # Convert generated image to HWC format\n\n#                 # Create a figure to display images\n#                 plt.figure(figsize=(15, 5))  # Set the figure size\n\n#                 plt.subplot(1, 3, 1)  # First subplot for SAR image\n#                 plt.imshow(sar_img_np)  # Display SAR image\n#                 plt.title('Original SAR Image')\n#                 plt.axis('off')  # Hide axis\n\n#                 plt.subplot(1, 3, 2)  # Second subplot for original optical image\n#                 plt.imshow(orig_optical_img_np)  # Display original optical image\n#                 plt.title('Original Optical Image')\n#                 plt.axis('off')  # Hide axis\n\n#                 plt.subplot(1, 3, 3)  # Third subplot for generated optical image\n#                 plt.imshow(gen_img_np)  # Display generated optical image\n#                 plt.title('Generated Optical Image')\n#                 plt.axis('off')  # Hide axis\n\n#                 plt.show()  # Show the figure\n\n#             if i + 1 >= num_images:  # Stop after processing the desired number of images\n#                 break\n\n# # Example usage (assuming test_loader is defined)\n# infer_sar_to_optical(test_loader, num_images=5)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T18:52:56.307750Z","iopub.execute_input":"2024-10-21T18:52:56.308292Z","iopub.status.idle":"2024-10-21T18:52:56.317516Z","shell.execute_reply.started":"2024-10-21T18:52:56.308245Z","shell.execute_reply":"2024-10-21T18:52:56.314981Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\n\n# Load the trained generator model\ncheckpoint = torch.load(\"/kaggle/input/check_point110.pth/pytorch/default/1/checkpoint_epoch_110.pth\")  # Load the entire checkpoint\ngenerator = UNetGenerator().to(device)  # Initialize the model\n\n# Load only the generator's state_dict\ngenerator.load_state_dict(checkpoint['generator_state_dict'])  # Load the state_dict for the generator\ngenerator.eval()\n\n# Function to perform inference and display SAR, original optical, and generated optical images\ndef infer_sar_to_optical(test_loader, num_images=5):\n    generator.eval()  # Set the generator to evaluation mode\n    with torch.no_grad():  # Disable gradient computation for inference\n        for i, (sar_images, optical_images) in enumerate(test_loader):  # Iterate through the test DataLoader\n            sar_images = sar_images.to(device)  # Move SAR images to the appropriate device\n            \n            # Generate optical images from SAR images\n            generated_images = generator(sar_images)  # Forward pass through the generator\n            \n            # Post-process the output\n            generated_images = generated_images.squeeze().cpu()  # Remove batch dimension and move to CPU\n            generated_images = generated_images.clamp(0, 1)  # Clamp values to the valid range [0, 1]\n\n            # Display SAR, original optical, and generated optical images side by side\n            for sar_img, orig_optical_img, gen_img in zip(sar_images.cpu(), optical_images.cpu(), generated_images):\n                sar_img_np = sar_img.squeeze().numpy().transpose(1, 2, 0)  # Convert SAR image to HWC format\n                orig_optical_img_np = orig_optical_img.squeeze().numpy().transpose(1, 2, 0)  # Convert original optical image to HWC format\n                gen_img_np = gen_img.numpy().transpose(1, 2, 0)  # Convert generated image to HWC format\n\n                # Create a figure to display images\n                plt.figure(figsize=(15, 5))  # Set the figure size\n\n                plt.subplot(1, 3, 1)  # First subplot for SAR image\n                plt.imshow(sar_img_np)  # Display SAR image\n                plt.title('Original SAR Image')\n                plt.axis('off')  # Hide axis\n\n                plt.subplot(1, 3, 2)  # Second subplot for original optical image\n                plt.imshow(orig_optical_img_np)  # Display original optical image\n                plt.title('Original Optical Image')\n                plt.axis('off')  # Hide axis\n\n                plt.subplot(1, 3, 3)  # Third subplot for generated optical image\n                plt.imshow(gen_img_np)  # Display generated optical image\n                plt.title('Generated Optical Image')\n                plt.axis('off')  # Hide axis\n\n                plt.show()  # Show the figure\n\n            if i + 1 >= num_images:  # Stop after processing the desired number of images\n                break\n\n# Example usage (assuming test_loader is defined)\ninfer_sar_to_optical(test_loader, num_images=5)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T18:52:56.318927Z","iopub.execute_input":"2024-10-21T18:52:56.324615Z","iopub.status.idle":"2024-10-21T18:53:49.092106Z","shell.execute_reply.started":"2024-10-21T18:52:56.324568Z","shell.execute_reply":"2024-10-21T18:53:49.091149Z"},"trusted":true},"outputs":[],"execution_count":null}]}